{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3e4402",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sWbXCGozBRNW",
   "metadata": {
    "id": "sWbXCGozBRNW"
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmxpysFu7zjH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmxpysFu7zjH",
    "outputId": "db2717d5-22be-4fa8-99fb-3f9ea90e7e1b",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# data elaboration functions\n",
    "import pandas as pd\n",
    "from six.moves import collections_abc\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "# datetime functions\n",
    "import datetime as dt\n",
    "\n",
    "# file management functions\n",
    "import os\n",
    "import sys\n",
    "import opendatasets as od\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# plot functions\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# data science functions\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, scale\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# statistical functions\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# configuration file\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# custom functions\n",
    "from Code.Profiling.Intermittent.intermittent import Intermittent\n",
    "from Code.Utils.utils import Utils\n",
    "from Code.Scoring.kpi import Kpi\n",
    "from Code.Scoring.forecast import Forecasting\n",
    "from Code.Scoring.train import Training\n",
    "from Code.Scoring.train_test import TrainTest\n",
    "from Code.Scoring.scoring import Scoring\n",
    "from Code.Regressors.regressors import Regressors\n",
    "from Code.Plotting.plots import Plots\n",
    "from Configuration.config import cfg_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc26b7b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458162d0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# od.download(\"https://www.kaggle.com/arashnic/building-sites-power-consumption-dataset/download\")\n",
    "root = Path(os.getcwd()).parent\n",
    "dataset_path = os.path.join(root, cfg_path.data_dir.input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb0e13",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09358d6d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dict_profiling = pd.read_pickle(os.path.join(root, cfg_path.data_dir.output_path, 'dict_profiling.pkl'))\n",
    "df_final = pd.read_pickle(os.path.join(\n",
    "    root, cfg_path.data_dir.output_path, 'df_final.pkl'))\n",
    "df_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ed7fb",
   "metadata": {},
   "source": [
    "## Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "id = 'site_id'\n",
    "list_unique_id = ['site_id', 'timestamp']\n",
    "list_temp = ['temp']\n",
    "y = 'value'\n",
    "date_var = Utils.find_date(df_final)\n",
    "\n",
    "# Winsorizing parameters\n",
    "highest = 0.05\n",
    "lowest = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering regular time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define regular ids list\n",
    "list_id_clustering = list(dict_profiling['regular'])\n",
    "mask = df_final[id].isin(list(dict_profiling['regular']))\n",
    "df = df_final.loc[mask, [date_var, id, y]]\n",
    "\n",
    "# Set seed\n",
    "sample_seed_kmeans = 789\n",
    "# Standardizing data\n",
    "df_win_sum = df.loc[:, [id, y]].groupby(id).apply(\n",
    "    lambda x: np.sum(winsorize(x, (highest, lowest)))).reset_index()\n",
    "df_win_sum.columns = [id, \"sum_\" + y]\n",
    "\n",
    "# Checking if some ids have 0 values after winsorizing\n",
    "if len(set(list_id_clustering) - set(list(df_win_sum[id].unique()))) > 0:\n",
    "    list_id_clustering = list(set(list_id_clustering) - set(list(df_win_sum[id].unique())))\n",
    "    print(id, list_id_clustering, \"has/have 0\", y, \"after winsorizing\")\n",
    "    mask = (df[y]!=np.nan) & (~df[id].isin(list_id_clustering))\n",
    "    df_std = df.loc[mask, ].pivot(index=date_var, columns=id, values=y).reset_index()\n",
    "    charvec = df_std[date_var].dt.strftime('%Y-%m-%d')\n",
    "    df_std.set_index(date_var, inplace=True)\n",
    "else:\n",
    "    mask = (df[y]!=np.nan)\n",
    "    df_std = df.loc[mask, ].pivot(index=date_var, columns=id, values=y).reset_index()\n",
    "    charvec = df_std[date_var].dt.strftime('%Y-%m-%d')\n",
    "    df_std.set_index(date_var, inplace=True)\n",
    "    print(\"NO\", id, \"has/have 0\", y, \"after winsorizing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a set of ids to cluster with NO nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to perform cluster analysis, one need to have a matrix with no nan value and set the index of the dataframe with date_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df_std_no_nan = df_std.dropna()\n",
    "if len(df_std_no_nan)==0:\n",
    "    list_id_cluster = [16, 21,22,25,26, 27, 29, 33, 40, 49]\n",
    "    df_cluster = df_std.loc[:, list_id_cluster].dropna()\n",
    "else:\n",
    "    list_id_cluster = list(set(list(df_std.columns)) - set(list(date_var)))\n",
    "    df_cluster = df_std.loc[:, list_id_cluster].dropna()\n",
    "print('Clustering regular profiles on ids', list_id_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the number of cluster you want to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Total sum of squares\n",
    "tot_ss = pd.DataFrame(df_cluster.apply(scale, axis=1)**2).sum(axis=0, skipna=True)\n",
    "\n",
    "# Setting up charvec\n",
    "start_date = min(df_cluster.index)\n",
    "end_date = max(df_cluster.index)\n",
    "\n",
    "# Define the number of clusters\n",
    "try_clusters = 11\n",
    "\n",
    "# K-means setup\n",
    "kmeans_kwargs = { \n",
    "    \"init\": \"random\",\n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 300,\n",
    "    \"random_state\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Appropriate Number of Clusters\n",
    "In this section, youâ€™ll look at two methods that are commonly used to evaluate the appropriate number of clusters:\n",
    "\n",
    "- The elbow method\n",
    "- The silhouette coefficient\n",
    "\n",
    "These are often used as complementary evaluation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#X = np.array(df_cluster.transpose())\n",
    "X = np.array(df_cluster)\n",
    "\n",
    "# A list holds the SSE values for each k\n",
    "\n",
    "sse = []\n",
    "for k in range(1, try_clusters):\n",
    "    kmeans = KMeans(n_clusters = k, **kmeans_kwargs)\n",
    "    kmeans.fit(X)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1, try_clusters), sse)\n",
    "plt.xticks(range(1, try_clusters))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "kl = KneeLocator(range(1, 11), sse, curve=\"convex\", direction=\"decreasing\")\n",
    "print(\"Elbow method: optimal number of clusters is\", kl.elbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The silhouette coefficient\n",
    "The silhouette coefficient is a measure of cluster cohesion and separation. It quantifies how well a data point fits into its assigned cluster based on two factors:\n",
    "\n",
    "- How close the data point is to other points in the cluster\n",
    "- How far away the data point is from points in other clusters\n",
    "\n",
    "Silhouette coefficient values range between -1 and 1. Larger numbers indicate that samples are closer to their clusters than they are to other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# A list holds the silhouette coefficients for each k\n",
    "silhouette_coefficients = []\n",
    "\n",
    "# Notice you start at 2 clusters for silhouette coefficient\n",
    "for k in range(2, try_clusters):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(X)\n",
    "    score = silhouette_score(X, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "    \n",
    "pd.DataFrame(silhouette_coefficients)\n",
    "    \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2, try_clusters), silhouette_coefficients)\n",
    "plt.xticks(range(2, try_clusters))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()\n",
    "\n",
    "df_sil_coeff = pd.DataFrame(silhouette_coefficients).reset_index()\n",
    "optimal_silhouette_coefficients = df_sil_coeff.loc[df_sil_coeff[0]==max(silhouette_coefficients), 'index'][0] + 2\n",
    "print(\"Silhouette coefficients: optimal number of clusters is\", optimal_silhouette_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering using the optimal number of clusters chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "chosen_clusters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=chosen_clusters, **kmeans_kwargs)\n",
    "identified_clusters = kmeans.fit_predict(X)\n",
    "\n",
    "df_cluster.loc[:, 'cluster'] = identified_clusters \n",
    "\n",
    "# Updating profiling dictionary\n",
    "dict_profiling['regular']['cluster'] = {}\n",
    "for c in range(0, len(dict_profiling['regular'])):\n",
    "    dict_profiling['cluster'] = {dict_profiling['regular'][c]: df_cluster.loc[df_cluster.index==dict_profiling['regular'][c], 'cluster'].unique()[0]}\n",
    "    print(id, c, dict_profiling[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting clustered regular series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df_to_plot = pd.melt(df_cluster.reset_index(), id_vars=[date_var, 'cluster'])\n",
    "for cluster in list(df_cluster['cluster'].unique()):\n",
    "    count = 1\n",
    "    for i in list(df_to_plot[id].unique()):\n",
    "        print('Plotting id:', i, 'as', count, 'of',\n",
    "              len(list(df_to_plot[id].unique())))\n",
    "        chart_title =  id + ' ' + str(i) + \" - Profile regular cluster \" +  str(cluster)\n",
    "        plot = Plots.sliding_line_plot(df_to_plot, y, id, i, chart_title)\n",
    "        plot.write_html(os.path.join(root, cfg_path.data_dir.plot_path,\n",
    "                        id + '_' + str(i) + '_profile_regular_cluster_' + str(cluster) + \".html\"))\n",
    "        count = count + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# create a binary pickle file \n",
    "f = open(os.path.join(root, cfg_path.data_dir.output_path, 'dict_profiling.pkl'),\"wb\")\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(dict_profiling,f)\n",
    "# close file\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61a9119356af14d297016aa436207c382cdf025efc60c362921907461dc4d044"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
